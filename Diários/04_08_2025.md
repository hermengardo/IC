# Diário (04/08/2025)

## Hoje

### Script para alocar tempo de GPU no cluster (jupyter notebook com GPU)
```
salloc --partition=partition --nodelist=node --time=00:30:00 # 30 minutos
srun --jobid=$ID --pty bash
ssh -L PORT:localhost:PORT -J USER@HOST USER@NODE
```

### Código de exemplo do EVO2
```python
  import torch
  from evo2 import Evo2
  
  evo2_model = Evo2('evo2_7b')
  
  sequence = 'ACGT'
  input_ids = torch.tensor(
      evo2_model.tokenizer.tokenize(sequence),
      dtype=torch.int,
  ).unsqueeze(0).to('cuda:0')
  
  layer_name = 'blocks.28.mlp.l3'
  
  outputs, embeddings = evo2_model(input_ids, return_embeddings=True, layer_names=[layer_name])
  
  print('Embeddings shape: ', embeddings[layer_name].shape)
```
- Gera o erro:
```python
  RuntimeError: /TransformerEngine/transformer_engine/common/gemm/cublaslt_gemm.cu:412 in function cublas_gemm: cuBLAS Error: an unsupported value or parameter was passed to the function
```

  - **Problema**: GPU é incompatível com FP8
  - **Na documentação:** Note that the 7B checkpoints can be run without FP8, thus avoiding the compute capability requirement. This can be done by modifying the configs to turn off FP8 and is not officially supported as there are numerical differences.
  - **Solução:**
    1. Em:
    ```python
      ./python3.12/site-packages/evo2/configs/evo2-7b-1m.yml
    ```
    2. Mudar o parâmetro `use_fp8_input_projections` para `False`.
      
- Para evitar:
```python
OutOfMemoryError: CUDA out of memory. Tried to allocate 176.00 MiB. GPU 0 has a total capacity of 44.42 GiB of which 1.38 MiB is free. Including non-PyTorch memory, this process has 44.41 GiB memory in use. Of the allocated memory 38.88 GiB is allocated by PyTorch, and 5.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
```
  - Usando:
    ```python
    import gc
    import torch
    
    gc.collect()
    torch.cuda.empty_cache()
    ```

### Para calcular os embeddings de uma determinada base, o evo2 considera todos os nucleotídeos na sequência ou todos os nucleotídeos até aquela posição?
> Evo2 models the joint probability of the sequence by factorizing it via the chain rule:
> $$p(x) = \prod_{t=1}^L (x_t|x_1, x_2, x_3, \dots, x_{t-1})$$
> This formulation is standard in autoregressive language models. For genomic sequences, it means that the probability of observing a nucleotide at a particular position is conditioned on the entire preceding context. [Preetham, 2025](https://medium.com/autonomous-agents/evo2-demystified-the-ultimate-technical-guide-to-genomic-language-modeling-a75b0afe7b87)

## Loss function do evo:
$$l_{wCE} = \frac{1}{Z}\sum_{t=1}^T w_tl_{CE} (x_t, y_t)$$

- $l_{CE} (x_t, y_t)$: é a entropia cruzada para a posição t.
- $w_t$ é um peso que ajusta a influencia do token com base em sua região (0.1 se é uma repetitive region ou 1.0 se é uma non-repetitive region).

## Rascunho de código para obter embeddings usando evo2
```python
class CFG:
    PATH = "seqs.parquet"
    LAYER = "blocks.28.mlp.l3"
    MODEL = "evo2_7b"
    SEQ_COL = "seq"

class Processor(object):
    def __init__(self) -> None:
        self.data = pl.read_parquet(CFG.PATH)
        self.layer = CFG.LAYER
        self.model = Evo2(CFG.MODEL)

        self.data = self.data.with_columns([
            pl
            .col([CFG.SEQ_COL])
            .map_elements(self.get_embeddings)
            .alias('embeddings')
        ])

    def get_embeddings(self, seq:str) -> list:
        seq = str(seq[0])
        
        inpt_ids = torch.tensor(
            self.model.tokenizer.tokenize(seq),
            dtype=torch.int,
        ).unsqueeze(0).to('cuda:0')

        _, embs = self.model(inpt_ids, return_embeddings=True, layer_names=[self.layer])

        embs = embs[self.layer]
        embs = embs.to(torch.float32)
        
        return embs.cpu().detach().numpy()
```

- **Input esperado para o modelo:** uma sequência com $n$ nucleotídeos.
- **Output esperado:** vetor contendo as probabilidades de seleção diversificadora para os $k$ códons da sequências.
- **Problema**: o output tem tamanho variável.
  
## Amanhã:
- [x] Identificar e resolver os erros.
- [x] Detalhar erro e solução no diário.
- [x] Terminar de configurar o ambiente.
- [ ] Escrever o script para a vetorização dos nucleotídeos.
- [x] Resolver o erro do evo2
- [ ] Continuar a processar os dados.
