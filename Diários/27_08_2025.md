# Criação das classes para vetorização não contextual
- Estou testando duas abordagens para a vetorização não contextual. A primeira utiliza o método CountVectorizer da biblioteca scikit-learn (bag-of-words) e a segunda utiliza uma abordagem própria.

# Vetorização usando `CountVectorizer`:
## Código
```python
class countVec():
    def __init__(self, 
                 loader:BatchLoader):
        
        self.data = loader.get_data
        self._vectorize()

    @property
    def get_embeddings(self):
        return self.data["embeddings"].clone() 

    def _vectorize(self) -> None:
        self.data = self.data.with_columns(
            pl
            .col("sequence")
            .map_elements(self._to_codon_string, return_dtype=str)
            .alias("codon str")
        ).clone()

        vectorizer = CountVectorizer()

        self.data = self.data.with_columns(
            pl.Series(
                "embeddings",
                vectorizer.fit_transform(
                    self.data["codon str"]
                ).toarray()
            )
        ).drop("codon str").clone()
        
    def _to_codon_string(self, seq:str) -> str:
        return " ".join([seq[i:i+3] for i in range(0, len(seq), 3)])
```    

## Pseudocódigo do método de vetorização:
```
    SEQUENCIA <- SEQUENCIAS PARA SEREM PROCESSADAS
    CODONS <- CONJUNTO VAZIO COM N POSIÇÕES
    PARA CADA SEQUÊNCIA EM SEQUENCIAS; FAÇA:
        CODONS_i <- SEPARA A SEQUÊNCIA USANDO 3-MERS
    FIM
    GERA UMA MATRIZ COM TODOS OS CODONS PRESENTES EM CODONS
    PARA CADA CODON EM CODONS:
        CONTA A FREQUÊNCIA DO CODON NA SEQUÊNCIA
        ATUALIZA A MATRIZ
    FIM
```

## Possíveis problemas: 
- O método `CountVectorizer` constrói a matriz (vocabulário) apenas com os dados fornecidos.
    1. Se os dados em treino possui códons não conhecidos $\implies$ o códon não estará representado

# Vetorização usando abordagem própria
## Ideia geral
Construir um vetor com 65 componentes.
- Cada componente representa uma combinação possível das 4 bases.
- O último componente contabiliza códons com bases não canônicas (menos frequentes).

## Código:
```python
class naiveVectorizer:
    def __init__(self, loader: BatchLoader):
        self.data = loader.get_data
        self._codon_dict = self._build_codon_dict()
        self._vectorize()

    @property
    def get_embeddings(self):
        return self.data["embeddings"].clone()

    def _build_codon_dict(self):
        bases = ["A", "T", "C", "G"]
        codons = ["".join(c) for c in product(bases, repeat=3)]
        return {codon: i for i, codon in enumerate(codons)}

    def _vectorize(self):
        self.data = self.data.with_columns(
            pl.col("sequence")
                .map_elements(
                    self._to_vector, 
                    return_dtype=object
                ).alias("embeddings")
        ).clone()

    def _to_vector(self, seq: str):
        vec = np.zeros(len(self._codon_dict) + 1, dtype=int)
        for i in range(0, len(seq), 3):
            codon = seq[i:i+3]
            if codon in self._codon_dict:
                vec[self._codon_dict[codon]] += 1
            else:
                vec[-1] += 1
        return vec
```

## Pseudocódigo 
```
    SEQUENCIA <- SEQUENCIAS PARA SEREM PROCESSADAS
    CODONS <- CONJUNTO VAZIO COM N POSIÇÕES
    CODONS_POSSIVEIS <- (PRODUTO_CARTESIANO(["A", "T", "C", "G"]), INDEX)

    PARA CADA SEQUÊNCIA EM SEQUENCIAS; FAÇA:
        CODONS_i <- SEPARA A SEQUÊNCIA USANDO 3-MERS

    V <- VETOR DE 65 ESPAÇOS IGUAIS A ZERO

    PARA CADA CODON EM CODONS:
        SE CODON ESTÁ EM CODONS_POSSIVEIS:
            SOMA 1 NA POSIÇÃO INDEX DE VETOR
        SENÃO
            SOMA 1 NA POSIÇÃO 65
    FIM
```

## Outras decisões
- Separar os conjuntos de treino, teste e validação em família $\implies$ Não deixar elementos da mesma família no mesmo conjunto.
- Para os casos sem árvores, gerar as árvores manualmente.

## Coisas para verificar
- Verificar os erros e a frequência deles no arquivo de log *hyphy*
