**Erro encontrado ao tentar gerar os embeddings:**

```python
RuntimeError: Numpy is not available
```

**Solução:**
```shell
pixi run pip install --force-reinstall --no-cache-dir --no-build-isolation transformer_engine
```

**Comecei a construir a classe para carregar os dados**
```python
class JsonlGzBatchLoader:
    def __init__(self) -> None:
        self.file_list = self._get_file_paths()
        self.zipped_batches = self._split_into_batches()
        self.data = self._read_all_files()
        
    def _unzip_file(self, filepath: str) -> List[str]:
        with gzip.open(filepath, 'rt', encoding='utf-8') as f:
            return f.readlines()

    def _read_all_files(self):
        batches = []
        with ThreadPoolExecutor(max_workers=CFG.JOBS) as executor:
            while len(self.zipped_batches) > 0:
                batch = self.zipped_batches.pop()
                    unziped = executor.map(
                        self._unzip_file,
                        batch
                    )
    
                    # data = [json.load(d) for d in unziped]
                    
                    batches.append(unziped)
                
        return batches

    def _split_into_batches(self) -> List[List[str]]:
        batches = []
        for i in range(0, len(self.file_list), 50):
            batches.append(self.file_list[i:i+50])
        return batches

    @staticmethod
    def _get_file_paths() -> List[str]:
        return [
            os.path.join(CFG.PATH, f)
            for f in os.listdir(CFG.PATH)
            if f.endswith('.jsonl.gz')
        ]
```

- Como otimizar esse processo?

**Pequenas alterações na classe para gerar os embeddings**
```python
class EmbeddingProcessor(object):
    def __init__(self) -> None:
        self.batches = JsonlGzBatchLoader
        
        self.layer = CFG.LAYER
        self.model = Evo2(CFG.MODEL)
        self.data = self.data.sample(5)

        self.data = self.data.with_columns([
            pl
            .col([CFG.SEQ_COL])
            .map_elements(
                self._embed_sequence
            )
            .alias('embeddings')
        ])

    def _embed_sequence(self, seq:str) -> list:
        seq = str(seq[0])
        
        inpt_ids = torch.tensor(
            self.model.tokenizer.tokenize(seq),
            dtype=torch.int,
        ).unsqueeze(0).to('cuda:0')

        _, embs = self.model(inpt_ids, return_embeddings=True, layer_names=[self.layer])

        embs = embs[self.layer]
        embs = embs.to(torch.float32).squeeze(0)
        
        return embs.cpu().detach().numpy()
```
